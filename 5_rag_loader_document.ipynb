{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document Loader sẽ giúp đọc các văn bản gồm:\n",
    "##### pdf, json, html, txt, md, ... dưới dạng Structured và Unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Load PDF File Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/pdf/1506.02640\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(url, extract_images=True)\n",
    "docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 0}, page_content='You Only Look Once:\\nUniﬁed, Real-Time Object Detection\\nJoseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\\nUniversity of Washington∗, Allen Institute for AI†, Facebook AI Research¶\\nhttp://pjreddie.com/yolo/\\nAbstract\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classiﬁers to per-\\nform detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and\\nassociated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from\\nfull images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end\\ndirectly on detection performance.\\nOur uniﬁed architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames\\nper second. A smaller version of the network, Fast YOLO,\\nprocesses an astounding 155 frames per second while\\nstill achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO\\nmakes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very\\ngeneral representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-\\nalizing from natural images to other domains like artwork.\\n1. Introduction\\nHumans glance at an image and instantly know what ob-\\njects are in the image, where they are, and how they inter-\\nact. The human visual system is fast and accurate, allow-\\ning us to perform complex tasks like driving with little con-\\nscious thought. Fast, accurate algorithms for object detec-\\ntion would allow computers to drive cars without special-\\nized sensors, enable assistive devices to convey real-time\\nscene information to human users, and unlock the potential\\nfor general purpose, responsive robotic systems.\\nCurrent detection systems repurpose classiﬁers to per-\\nform detection. To detect an object, these systems take a\\nclassiﬁer for that object and evaluate it at various locations\\nand scales in a test image. Systems like deformable parts\\nmodels (DPM) use a sliding window approach where the\\nclassiﬁer is run at evenly spaced locations over the entire\\nimage [10].\\nMore recent approaches like R-CNN use region proposal\\n1. Resize image.\\n2. Run convolutional network.3. Non-max suppression.\\nDog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\\nwith YOLO is simple and straightforward. Our system (1) resizes\\nthe input image to 448×448, (2) runs a single convolutional net-\\nwork on the image, and (3) thresholds the resulting detections by\\nthe model’s conﬁdence.\\nmethods to ﬁrst generate potential bounding boxes in an im-\\nage and then run a classiﬁer on these proposed boxes. After\\nclassiﬁcation, post-processing is used to reﬁne the bound-\\ning boxes, eliminate duplicate detections, and rescore the\\nboxes based on other objects in the scene [13]. These com-\\nplex pipelines are slow and hard to optimize because each\\nindividual component must be trained separately.\\nWe reframe object detection as a single regression prob-\\nlem, straight from image pixels to bounding box coordi-\\nnates and class probabilities. Using our system, you only\\nlook once (YOLO) at an image to predict what objects are\\npresent and where they are.\\nYOLO is refreshingly simple: see Figure 1. A sin-\\ngle convolutional network simultaneously predicts multi-\\nple bounding boxes and class probabilities for those boxes.\\nYOLO trains on full images and directly optimizes detec-\\ntion performance. This uniﬁed model has several beneﬁts\\nover traditional methods of object detection.\\nFirst, YOLO is extremely fast. Since we frame detection\\nas a regression problem we don’t need a complex pipeline.\\nWe simply run our neural network on a new image at test\\ntime to predict detections. Our base network runs at 45\\nframes per second with no batch processing on a Titan X\\nGPU and a fast version runs at more than 150 fps. This\\nmeans we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/ .\\nSecond, YOLO reasons globally about the image when\\n1arXiv:1506.02640v5  [cs.CV]  9 May 2016'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 1}, page_content='making predictions. Unlike sliding window and region\\nproposal-based techniques, YOLO sees the entire image\\nduring training and test time so it implicitly encodes contex-\\ntual information about classes as well as their appearance.\\nFast R-CNN, a top detection method [14], mistakes back-\\nground patches in an image for objects because it can’t see\\nthe larger context. YOLO makes less than half the number\\nof background errors compared to Fast R-CNN.\\nThird, YOLO learns generalizable representations of ob-\\njects. When trained on natural images and tested on art-\\nwork, YOLO outperforms top detection methods like DPM\\nand R-CNN by a wide margin. Since YOLO is highly gen-\\neralizable it is less likely to break down when applied to\\nnew domains or unexpected inputs.\\nYOLO still lags behind state-of-the-art detection systems\\nin accuracy. While it can quickly identify objects in im-\\nages it struggles to precisely localize some objects, espe-\\ncially small ones. We examine these tradeoffs further in our\\nexperiments.\\nAll of our training and testing code is open source. A\\nvariety of pretrained models are also available to download.\\n2. Uniﬁed Detection\\nWe unify the separate components of object detection\\ninto a single neural network. Our network uses features\\nfrom the entire image to predict each bounding box. It also\\npredicts all bounding boxes across all classes for an im-\\nage simultaneously. This means our network reasons glob-\\nally about the full image and all the objects in the image.\\nThe YOLO design enables end-to-end training and real-\\ntime speeds while maintaining high average precision.\\nOur system divides the input image into an S×Sgrid.\\nIf the center of an object falls into a grid cell, that grid cell\\nis responsible for detecting that object.\\nEach grid cell predicts Bbounding boxes and conﬁdence\\nscores for those boxes. These conﬁdence scores reﬂect how\\nconﬁdent the model is that the box contains an object and\\nalso how accurate it thinks the box is that it predicts. For-\\nmally we deﬁne conﬁdence as Pr(Object )∗IOUtruth\\npred. If no\\nobject exists in that cell, the conﬁdence scores should be\\nzero. Otherwise we want the conﬁdence score to equal the\\nintersection over union (IOU) between the predicted box\\nand the ground truth.\\nEach bounding box consists of 5 predictions: x,y,w,h,\\nand conﬁdence. The (x,y)coordinates represent the center\\nof the box relative to the bounds of the grid cell. The width\\nand height are predicted relative to the whole image. Finally\\nthe conﬁdence prediction represents the IOU between the\\npredicted box and any ground truth box.\\nEach grid cell also predicts Cconditional class proba-\\nbilities, Pr(Classi|Object ). These probabilities are condi-\\ntioned on the grid cell containing an object. We only predictone set of class probabilities per grid cell, regardless of the\\nnumber of boxes B.\\nAt test time we multiply the conditional class probabili-\\nties and the individual box conﬁdence predictions,\\nPr(Classi|Object )∗Pr(Object )∗IOUtruth\\npred= Pr( Classi)∗IOUtruth\\npred(1)\\nwhich gives us class-speciﬁc conﬁdence scores for each\\nbox. These scores encode both the probability of that class\\nappearing in the box and how well the predicted box ﬁts the\\nobject.\\nS × S grid on inputBounding boxes + confidence\\nClass probability mapFinal detections\\nFigure 2: The Model. Our system models detection as a regres-\\nsion problem. It divides the image into an S×Sgrid and for each\\ngrid cell predicts Bbounding boxes, conﬁdence for those boxes,\\nandCclass probabilities. These predictions are encoded as an\\nS×S×(B∗5 +C)tensor.\\nFor evaluating YOLO on P ASCAL VOC, we use S= 7,\\nB= 2. PASCAL VOC has 20 labelled classes so C= 20 .\\nOur ﬁnal prediction is a 7×7×30tensor.\\n2.1. Network Design\\nWe implement this model as a convolutional neural net-\\nwork and evaluate it on the P ASCAL VOC detection dataset\\n[9]. The initial convolutional layers of the network extract\\nfeatures from the image while the fully connected layers\\npredict the output probabilities and coordinates.\\nOur network architecture is inspired by the GoogLeNet\\nmodel for image classiﬁcation [34]. Our network has 24\\nconvolutional layers followed by 2 fully connected layers.\\nInstead of the inception modules used by GoogLeNet, we\\nsimply use 1×1reduction layers followed by 3×3convo-\\nlutional layers, similar to Lin et al [22]. The full network is\\nshown in Figure 3.\\nWe also train a fast version of YOLO designed to push\\nthe boundaries of fast object detection. Fast YOLO uses a\\nneural network with fewer convolutional layers (9 instead\\nof 24) and fewer ﬁlters in those layers. Other than the size\\nof the network, all training and testing parameters are the\\nsame between YOLO and Fast YOLO.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 2}, page_content='448\\n448\\n3\\n7\\n7\\nConv. Layer\\n7x7x64-s-2\\nMaxpool Layer\\n2x2-s-2\\n3\\n3112\\n112\\n192\\n3\\n356\\n56\\n256\\nConn. Layer4096\\nConn. Layer Conv. Layer\\n3x3x192\\nMaxpool Layer\\n2x2-s-2Conv. Layers\\n1x1x128\\n3x3x256\\n1x1x256\\n3x3x512\\nMaxpool Layer\\n2x2-s-2\\n3\\n328\\n28\\n512\\nConv. Layers\\n1x1x256\\n3x3x5121x1x512\\n3x3x1024\\nMaxpool Layer\\n2x2-s-2\\n3\\n314\\n14\\n1024\\nConv. Layers\\n1x1x512\\n3x3x10243x3x1024\\n3x3x1024-s-2\\n3\\n37\\n7\\n10247\\n7\\n10247\\n7\\n30\\n} ×4 } ×2Conv. Layers\\n3x3x1024\\n3x3x1024Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1×1\\nconvolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classiﬁcation\\ntask at half the resolution ( 224×224input image) and then double the resolution for detection.\\nThe ﬁnal output of our network is the 7×7×30tensor\\nof predictions.\\n2.2. Training\\nWe pretrain our convolutional layers on the ImageNet\\n1000-class competition dataset [30]. For pretraining we use\\nthe ﬁrst 20 convolutional layers from Figure 3 followed by a\\naverage-pooling layer and a fully connected layer. We train\\nthis network for approximately a week and achieve a single\\ncrop top-5 accuracy of 88% on the ImageNet 2012 valida-\\ntion set, comparable to the GoogLeNet models in Caffe’s\\nModel Zoo [24]. We use the Darknet framework for all\\ntraining and inference [26].\\nWe then convert the model to perform detection. Ren et\\nal. show that adding both convolutional and connected lay-\\ners to pretrained networks can improve performance [29].\\nFollowing their example, we add four convolutional lay-\\ners and two fully connected layers with randomly initialized\\nweights. Detection often requires ﬁne-grained visual infor-\\nmation so we increase the input resolution of the network\\nfrom 224×224to448×448.\\nOur ﬁnal layer predicts both class probabilities and\\nbounding box coordinates. We normalize the bounding box\\nwidth and height by the image width and height so that they\\nfall between 0 and 1. We parametrize the bounding box x\\nandycoordinates to be offsets of a particular grid cell loca-\\ntion so they are also bounded between 0 and 1.\\nWe use a linear activation function for the ﬁnal layer and\\nall other layers use the following leaky rectiﬁed linear acti-\\nvation:\\nφ(x) ={\\nx, ifx>0\\n0.1x,otherwise(2)\\nWe optimize for sum-squared error in the output of ourmodel. We use sum-squared error because it is easy to op-\\ntimize, however it does not perfectly align with our goal of\\nmaximizing average precision. It weights localization er-\\nror equally with classiﬁcation error which may not be ideal.\\nAlso, in every image many grid cells do not contain any\\nobject. This pushes the “conﬁdence” scores of those cells\\ntowards zero, often overpowering the gradient from cells\\nthat do contain objects. This can lead to model instability,\\ncausing training to diverge early on.\\nTo remedy this, we increase the loss from bounding box\\ncoordinate predictions and decrease the loss from conﬁ-\\ndence predictions for boxes that don’t contain objects. We\\nuse two parameters, λcoordandλnoobjto accomplish this. We\\nsetλcoord= 5andλnoobj=.5.\\nSum-squared error also equally weights errors in large\\nboxes and small boxes. Our error metric should reﬂect that\\nsmall deviations in large boxes matter less than in small\\nboxes. To partially address this we predict the square root\\nof the bounding box width and height instead of the width\\nand height directly.\\nYOLO predicts multiple bounding boxes per grid cell.\\nAt training time we only want one bounding box predictor\\nto be responsible for each object. We assign one predictor\\nto be “responsible” for predicting an object based on which\\nprediction has the highest current IOU with the ground\\ntruth. This leads to specialization between the bounding box\\npredictors. Each predictor gets better at predicting certain\\nsizes, aspect ratios, or classes of object, improving overall\\nrecall.\\nDuring training we optimize the following, multi-part'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 3}, page_content='loss function:\\nλcoordS2∑\\ni=0B∑\\nj=01obj\\nij[\\n(xi−ˆxi)2+ (yi−ˆyi)2]\\n+λcoordS2∑\\ni=0B∑\\nj=01obj\\nij[(√wi−√\\nˆwi)2+(√\\nhi−√\\nˆhi)2]\\n+S2∑\\ni=0B∑\\nj=01obj\\nij(\\nCi−ˆCi)2\\n+λnoobjS2∑\\ni=0B∑\\nj=01noobj\\nij(\\nCi−ˆCi)2\\n+S2∑\\ni=01obj\\ni∑\\nc∈classes(pi(c)−ˆpi(c))2(3)\\nwhere 1obj\\nidenotes if object appears in cell iand 1obj\\nijde-\\nnotes that the jth bounding box predictor in cell iis “re-\\nsponsible” for that prediction.\\nNote that the loss function only penalizes classiﬁcation\\nerror if an object is present in that grid cell (hence the con-\\nditional class probability discussed earlier). It also only pe-\\nnalizes bounding box coordinate error if that predictor is\\n“responsible” for the ground truth box (i.e. has the highest\\nIOU of any predictor in that grid cell).\\nWe train the network for about 135 epochs on the train-\\ning and validation data sets from P ASCAL VOC 2007 and\\n2012. When testing on 2012 we also include the VOC 2007\\ntest data for training. Throughout training we use a batch\\nsize of 64, a momentum of 0.9and a decay of 0.0005 .\\nOur learning rate schedule is as follows: For the ﬁrst\\nepochs we slowly raise the learning rate from 10−3to10−2.\\nIf we start at a high learning rate our model often diverges\\ndue to unstable gradients. We continue training with 10−2\\nfor 75 epochs, then 10−3for 30 epochs, and ﬁnally 10−4\\nfor 30 epochs.\\nTo avoid overﬁtting we use dropout and extensive data\\naugmentation. A dropout layer with rate = .5 after the ﬁrst\\nconnected layer prevents co-adaptation between layers [18].\\nFor data augmentation we introduce random scaling and\\ntranslations of up to 20% of the original image size. We\\nalso randomly adjust the exposure and saturation of the im-\\nage by up to a factor of 1.5in the HSV color space.\\n2.3. Inference\\nJust like in training, predicting detections for a test image\\nonly requires one network evaluation. On P ASCAL VOC the\\nnetwork predicts 98 bounding boxes per image and class\\nprobabilities for each box. YOLO is extremely fast at test\\ntime since it only requires a single network evaluation, un-\\nlike classiﬁer-based methods.\\nThe grid design enforces spatial diversity in the bound-\\ning box predictions. Often it is clear which grid cell an\\nobject falls in to and the network only predicts one box for\\neach object. However, some large objects or objects nearthe border of multiple cells can be well localized by multi-\\nple cells. Non-maximal suppression can be used to ﬁx these\\nmultiple detections. While not critical to performance as it\\nis for R-CNN or DPM, non-maximal suppression adds 2-\\n3% in mAP.\\n2.4. Limitations of YOLO\\nYOLO imposes strong spatial constraints on bounding\\nbox predictions since each grid cell only predicts two boxes\\nand can only have one class. This spatial constraint lim-\\nits the number of nearby objects that our model can pre-\\ndict. Our model struggles with small objects that appear in\\ngroups, such as ﬂocks of birds.\\nSince our model learns to predict bounding boxes from\\ndata, it struggles to generalize to objects in new or unusual\\naspect ratios or conﬁgurations. Our model also uses rela-\\ntively coarse features for predicting bounding boxes since\\nour architecture has multiple downsampling layers from the\\ninput image.\\nFinally, while we train on a loss function that approxi-\\nmates detection performance, our loss function treats errors\\nthe same in small bounding boxes versus large bounding\\nboxes. A small error in a large box is generally benign but a\\nsmall error in a small box has a much greater effect on IOU.\\nOur main source of error is incorrect localizations.\\n3. Comparison to Other Detection Systems\\nObject detection is a core problem in computer vision.\\nDetection pipelines generally start by extracting a set of\\nrobust features from input images (Haar [25], SIFT [23],\\nHOG [4], convolutional features [6]). Then, classiﬁers\\n[36, 21, 13, 10] or localizers [1, 32] are used to identify\\nobjects in the feature space. These classiﬁers or localizers\\nare run either in sliding window fashion over the whole im-\\nage or on some subset of regions in the image [35, 15, 39].\\nWe compare the YOLO detection system to several top de-\\ntection frameworks, highlighting key similarities and differ-\\nences.\\nDeformable parts models. Deformable parts models\\n(DPM) use a sliding window approach to object detection\\n[10]. DPM uses a disjoint pipeline to extract static features,\\nclassify regions, predict bounding boxes for high scoring\\nregions, etc. Our system replaces all of these disparate parts\\nwith a single convolutional neural network. The network\\nperforms feature extraction, bounding box prediction, non-\\nmaximal suppression, and contextual reasoning all concur-\\nrently. Instead of static features, the network trains the fea-\\ntures in-line and optimizes them for the detection task. Our\\nuniﬁed architecture leads to a faster, more accurate model\\nthan DPM.\\nR-CNN. R-CNN and its variants use region proposals in-\\nstead of sliding windows to ﬁnd objects in images. Selective'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 4}, page_content='Search [35] generates potential bounding boxes, a convolu-\\ntional network extracts features, an SVM scores the boxes, a\\nlinear model adjusts the bounding boxes, and non-max sup-\\npression eliminates duplicate detections. Each stage of this\\ncomplex pipeline must be precisely tuned independently\\nand the resulting system is very slow, taking more than 40\\nseconds per image at test time [14].\\nYOLO shares some similarities with R-CNN. Each grid\\ncell proposes potential bounding boxes and scores those\\nboxes using convolutional features. However, our system\\nputs spatial constraints on the grid cell proposals which\\nhelps mitigate multiple detections of the same object. Our\\nsystem also proposes far fewer bounding boxes, only 98\\nper image compared to about 2000 from Selective Search.\\nFinally, our system combines these individual components\\ninto a single, jointly optimized model.\\nOther Fast Detectors Fast and Faster R-CNN focus on\\nspeeding up the R-CNN framework by sharing computa-\\ntion and using neural networks to propose regions instead\\nof Selective Search [14] [28]. While they offer speed and\\naccuracy improvements over R-CNN, both still fall short of\\nreal-time performance.\\nMany research efforts focus on speeding up the DPM\\npipeline [31] [38] [5]. They speed up HOG computation,\\nuse cascades, and push computation to GPUs. However,\\nonly 30Hz DPM [31] actually runs in real-time.\\nInstead of trying to optimize individual components of\\na large detection pipeline, YOLO throws out the pipeline\\nentirely and is fast by design.\\nDetectors for single classes like faces or people can be\\nhighly optimized since they have to deal with much less\\nvariation [37]. YOLO is a general purpose detector that\\nlearns to detect a variety of objects simultaneously.\\nDeep MultiBox. Unlike R-CNN, Szegedy et al. train a\\nconvolutional neural network to predict regions of interest\\n[8] instead of using Selective Search. MultiBox can also\\nperform single object detection by replacing the conﬁdence\\nprediction with a single class prediction. However, Multi-\\nBox cannot perform general object detection and is still just\\na piece in a larger detection pipeline, requiring further im-\\nage patch classiﬁcation. Both YOLO and MultiBox use a\\nconvolutional network to predict bounding boxes in an im-\\nage but YOLO is a complete detection system.\\nOverFeat. Sermanet et al. train a convolutional neural\\nnetwork to perform localization and adapt that localizer to\\nperform detection [32]. OverFeat efﬁciently performs slid-\\ning window detection but it is still a disjoint system. Over-\\nFeat optimizes for localization, not detection performance.\\nLike DPM, the localizer only sees local information when\\nmaking a prediction. OverFeat cannot reason about global\\ncontext and thus requires signiﬁcant post-processing to pro-\\nduce coherent detections.\\nMultiGrasp. Our work is similar in design to work ongrasp detection by Redmon et al [27]. Our grid approach to\\nbounding box prediction is based on the MultiGrasp system\\nfor regression to grasps. However, grasp detection is a much\\nsimpler task than object detection. MultiGrasp only needs\\nto predict a single graspable region for an image containing\\none object. It doesn’t have to estimate the size, location,\\nor boundaries of the object or predict it’s class, only ﬁnd a\\nregion suitable for grasping. YOLO predicts both bounding\\nboxes and class probabilities for multiple objects of multi-\\nple classes in an image.\\n4. Experiments\\nFirst we compare YOLO with other real-time detection\\nsystems on P ASCAL VOC 2007. To understand the differ-\\nences between YOLO and R-CNN variants we explore the\\nerrors on VOC 2007 made by YOLO and Fast R-CNN, one\\nof the highest performing versions of R-CNN [14]. Based\\non the different error proﬁles we show that YOLO can be\\nused to rescore Fast R-CNN detections and reduce the er-\\nrors from background false positives, giving a signiﬁcant\\nperformance boost. We also present VOC 2012 results and\\ncompare mAP to current state-of-the-art methods. Finally,\\nwe show that YOLO generalizes to new domains better than\\nother detectors on two artwork datasets.\\n4.1. Comparison to Other Real-Time Systems\\nMany research efforts in object detection focus on mak-\\ning standard detection pipelines fast. [5] [38] [31] [14] [17]\\n[28] However, only Sadeghi et al. actually produce a de-\\ntection system that runs in real-time (30 frames per second\\nor better) [31]. We compare YOLO to their GPU imple-\\nmentation of DPM which runs either at 30Hz or 100Hz.\\nWhile the other efforts don’t reach the real-time milestone\\nwe also compare their relative mAP and speed to examine\\nthe accuracy-performance tradeoffs available in object de-\\ntection systems.\\nFast YOLO is the fastest object detection method on\\nPASCAL ; as far as we know, it is the fastest extant object\\ndetector. With 52.7%mAP, it is more than twice as accurate\\nas prior work on real-time detection. YOLO pushes mAP to\\n63.4%while still maintaining real-time performance.\\nWe also train YOLO using VGG-16. This model is more\\naccurate but also signiﬁcantly slower than YOLO. It is use-\\nful for comparison to other detection systems that rely on\\nVGG-16 but since it is slower than real-time the rest of the\\npaper focuses on our faster models.\\nFastest DPM effectively speeds up DPM without sacri-\\nﬁcing much mAP but it still misses real-time performance\\nby a factor of 2 [38]. It also is limited by DPM’s relatively\\nlow accuracy on detection compared to neural network ap-\\nproaches.\\nR-CNN minus R replaces Selective Search with static\\nbounding box proposals [20]. While it is much faster than'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 5}, page_content='Real-Time Detectors Train mAP FPS\\n100Hz DPM [31] 2007 16.0 100\\n30Hz DPM [31] 2007 26.1 30\\nFast YOLO 2007+2012 52.7 155\\nYOLO 2007+2012 63.4 45\\nLess Than Real-Time\\nFastest DPM [38] 2007 30.4 15\\nR-CNN Minus R [20] 2007 53.5 6\\nFast R-CNN [14] 2007+2012 70.0 0.5\\nFaster R-CNN VGG-16[28] 2007+2012 73.2 7\\nFaster R-CNN ZF [28] 2007+2012 62.1 18\\nYOLO VGG-16 2007+2012 66.4 21\\nTable 1: Real-Time Systems on P ASCAL VOC 2007. Compar-\\ning the performance and speed of fast detectors. Fast YOLO is\\nthe fastest detector on record for P ASCAL VOC detection and is\\nstill twice as accurate as any other real-time detector. YOLO is\\n10 mAP more accurate than the fast version while still well above\\nreal-time in speed.\\nR-CNN, it still falls short of real-time and takes a signiﬁcant\\naccuracy hit from not having good proposals.\\nFast R-CNN speeds up the classiﬁcation stage of R-CNN\\nbut it still relies on selective search which can take around\\n2 seconds per image to generate bounding box proposals.\\nThus it has high mAP but at 0.5fps it is still far from real-\\ntime.\\nThe recent Faster R-CNN replaces selective search with\\na neural network to propose bounding boxes, similar to\\nSzegedy et al. [8] In our tests, their most accurate model\\nachieves 7 fps while a smaller, less accurate one runs at\\n18 fps. The VGG-16 version of Faster R-CNN is 10 mAP\\nhigher but is also 6 times slower than YOLO. The Zeiler-\\nFergus Faster R-CNN is only 2.5 times slower than YOLO\\nbut is also less accurate.\\n4.2. VOC 2007 Error Analysis\\nTo further examine the differences between YOLO and\\nstate-of-the-art detectors, we look at a detailed breakdown\\nof results on VOC 2007. We compare YOLO to Fast R-\\nCNN since Fast R-CNN is one of the highest performing\\ndetectors on P ASCAL and it’s detections are publicly avail-\\nable.\\nWe use the methodology and tools of Hoiem et al. [19]\\nFor each category at test time we look at the top N predic-\\ntions for that category. Each prediction is either correct or\\nit is classiﬁed based on the type of error:\\n•Correct: correct class and IOU >.5\\n•Localization: correct class, .1<IOU<.5\\n•Similar: class is similar, IOU >.1\\nCorrect: 71.6% Correct: 65.5%Loc: 8.6%Sim: 4.3%Other: 1.9%Background: 13.6%\\nLoc: 19.0%Sim: 6.75%Other: 4.0%Background: 4.75%Fast R-CNN YOLOFigure 4: Error Analysis: Fast R-CNN vs. YOLO These\\ncharts show the percentage of localization and background errors\\nin the top N detections for various categories (N = # objects in that\\ncategory).\\n•Other: class is wrong, IOU >.1\\n•Background: IOU <.1for any object\\nFigure 4 shows the breakdown of each error type aver-\\naged across all 20 classes.\\nYOLO struggles to localize objects correctly. Localiza-\\ntion errors account for more of YOLO’s errors than all other\\nsources combined. Fast R-CNN makes much fewer local-\\nization errors but far more background errors. 13.6% of\\nit’s top detections are false positives that don’t contain any\\nobjects. Fast R-CNN is almost 3x more likely to predict\\nbackground detections than YOLO.\\n4.3. Combining Fast R-CNN and YOLO\\nYOLO makes far fewer background mistakes than Fast\\nR-CNN. By using YOLO to eliminate background detec-\\ntions from Fast R-CNN we get a signiﬁcant boost in perfor-\\nmance. For every bounding box that R-CNN predicts we\\ncheck to see if YOLO predicts a similar box. If it does, we\\ngive that prediction a boost based on the probability pre-\\ndicted by YOLO and the overlap between the two boxes.\\nThe best Fast R-CNN model achieves a mAP of 71.8%\\non the VOC 2007 test set. When combined with YOLO, its\\nmAP Combined Gain\\nFast R-CNN 71.8 - -\\nFast R-CNN (2007 data) 66.9 72.4 .6\\nFast R-CNN (VGG-M) 59.2 72.4 .6\\nFast R-CNN (CaffeNet) 57.1 72.1 .3\\nYOLO 63.4 75.0 3.2\\nTable 2: Model combination experiments on VOC 2007. We\\nexamine the effect of combining various models with the best ver-\\nsion of Fast R-CNN. Other versions of Fast R-CNN provide only\\na small beneﬁt while YOLO provides a signiﬁcant performance\\nboost.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 6}, page_content='VOC 2012 test mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike personplant sheep sofa train tv\\nMR CNN MORE DATA [11] 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0\\nHyperNet VGG 71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7\\nHyperNet SP 71.3 84.1 78.3 73.3 55.5 53.6 78.6 79.6 87.5 49.5 74.9 52.1 85.6 81.6 83.2 81.6 48.4 73.2 59.3 79.7 65.6\\nFast R-CNN + YOLO 70.7 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2\\nMR CNN SCNN [11] 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1\\nFaster R-CNN [28] 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\\nDEEP ENS COCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4\\nNoC [29] 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1\\nFast R-CNN [14] 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2\\nUMICH FGS STRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2\\nNUS NIN C2000 [7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3\\nBabyLearning [7] 63.2 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6\\nNUS NIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7\\nR-CNN VGG BB [13] 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3\\nR-CNN VGG [13] 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7\\nYOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8\\nFeature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8\\nR-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1\\nSDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7\\nR-CNN [13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6\\nTable 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of\\nNovember 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the\\nonly real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.\\nmAP increases by 3.2% to 75.0%. We also tried combining\\nthe top Fast R-CNN model with several other versions of\\nFast R-CNN. Those ensembles produced small increases in\\nmAP between .3 and .6%, see Table 2 for details.\\nThe boost from YOLO is not simply a byproduct of\\nmodel ensembling since there is little beneﬁt from combin-\\ning different versions of Fast R-CNN. Rather, it is precisely\\nbecause YOLO makes different kinds of mistakes at test\\ntime that it is so effective at boosting Fast R-CNN’s per-\\nformance.\\nUnfortunately, this combination doesn’t beneﬁt from the\\nspeed of YOLO since we run each model seperately and\\nthen combine the results. However, since YOLO is so fast\\nit doesn’t add any signiﬁcant computational time compared\\nto Fast R-CNN.\\n4.4. VOC 2012 Results\\nOn the VOC 2012 test set, YOLO scores 57.9% mAP.\\nThis is lower than the current state of the art, closer to\\nthe original R-CNN using VGG-16, see Table 3. Our sys-\\ntem struggles with small objects compared to its closest\\ncompetitors. On categories like bottle ,sheep , and\\ntv/monitor YOLO scores 8-10% lower than R-CNN or\\nFeature Edit. However, on other categories like cat and\\ntrain YOLO achieves higher performance.\\nOur combined Fast R-CNN + YOLO model is one of the\\nhighest performing detection methods. Fast R-CNN gets\\na 2.3% improvement from the combination with YOLO,\\nboosting it 5 spots up on the public leaderboard.\\n4.5. Generalizability: Person Detection in Artwork\\nAcademic datasets for object detection draw the training\\nand testing data from the same distribution. In real-world\\napplications it is hard to predict all possible use cases andthe test data can diverge from what the system has seen be-\\nfore [3]. We compare YOLO to other detection systems on\\nthe Picasso Dataset [12] and the People-Art Dataset [3], two\\ndatasets for testing person detection on artwork.\\nFigure 5 shows comparative performance between\\nYOLO and other detection methods. For reference, we give\\nVOC 2007 detection AP on person where all models are\\ntrained only on VOC 2007 data. On Picasso models are\\ntrained on VOC 2012 while on People-Art they are trained\\non VOC 2010.\\nR-CNN has high AP on VOC 2007. However, R-CNN\\ndrops off considerably when applied to artwork. R-CNN\\nuses Selective Search for bounding box proposals which is\\ntuned for natural images. The classiﬁer step in R-CNN only\\nsees small regions and needs good proposals.\\nDPM maintains its AP well when applied to artwork.\\nPrior work theorizes that DPM performs well because it has\\nstrong spatial models of the shape and layout of objects.\\nThough DPM doesn’t degrade as much as R-CNN, it starts\\nfrom a lower AP.\\nYOLO has good performance on VOC 2007 and its AP\\ndegrades less than other methods when applied to artwork.\\nLike DPM, YOLO models the size and shape of objects,\\nas well as relationships between objects and where objects\\ncommonly appear. Artwork and natural images are very\\ndifferent on a pixel level but they are similar in terms of\\nthe size and shape of objects, thus YOLO can still predict\\ngood bounding boxes and detections.\\n5. Real-Time Detection In The Wild\\nYOLO is a fast, accurate object detector, making it ideal\\nfor computer vision applications. We connect YOLO to a\\nwebcam and verify that it maintains real-time performance,'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 7}, page_content='Poselets\\nRCNN\\nD&THumans\\nDPMYOLO\\n(a)Picasso Dataset precision-recall curves.VOC 2007 Picasso People-Art\\nAP AP BestF1 AP\\nYOLO 59.2 53.3 0.590 45\\nR-CNN 54.2 10.4 0.226 26\\nDPM 43.2 37.8 0.458 32\\nPoselets [2] 36.5 17.8 0.271\\nD&T [4] - 1.9 0.051\\n(b)Quantitative results on the VOC 2007, Picasso, and People-Art Datasets.\\nThe Picasso Dataset evaluates on both AP and best F1score.\\nFigure 5: Generalization results on Picasso and People-Art datasets.\\nFigure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\\ndoes think one person is an airplane.\\nincluding the time to fetch images from the camera and dis-\\nplay the detections.\\nThe resulting system is interactive and engaging. While\\nYOLO processes images individually, when attached to a\\nwebcam it functions like a tracking system, detecting ob-\\njects as they move around and change in appearance. A\\ndemo of the system and the source code can be found on\\nour project website: http://pjreddie.com/yolo/ .\\n6. Conclusion\\nWe introduce YOLO, a uniﬁed model for object detec-\\ntion. Our model is simple to construct and can be traineddirectly on full images. Unlike classiﬁer-based approaches,\\nYOLO is trained on a loss function that directly corresponds\\nto detection performance and the entire model is trained\\njointly.\\nFast YOLO is the fastest general-purpose object detec-\\ntor in the literature and YOLO pushes the state-of-the-art in\\nreal-time object detection. YOLO also generalizes well to\\nnew domains making it ideal for applications that rely on\\nfast, robust object detection.\\nAcknowledgements: This work is partially supported by\\nONR N00014-13-1-0720, NSF IIS-1338054, and The Allen\\nDistinguished Investigator Award.'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 8}, page_content='References\\n[1] M. B. Blaschko and C. H. Lampert. Learning to localize ob-\\njects with structured output regression. In Computer Vision–\\nECCV 2008 , pages 2–15. Springer, 2008. 4\\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors\\ntrained using 3d human pose annotations. In International\\nConference on Computer Vision (ICCV) , 2009. 8\\n[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-\\ndepiction problem: Computer vision algorithms for recog-\\nnising objects in artwork and in photographs. arXiv preprint\\narXiv:1505.00110 , 2015. 7\\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In Computer Vision and Pattern Recogni-\\ntion, 2005. CVPR 2005. IEEE Computer Society Conference\\non, volume 1, pages 886–893. IEEE, 2005. 4, 8\\n[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, J. Yagnik, et al. Fast, accurate detection of\\n100,000 object classes on a single machine. In Computer\\nVision and Pattern Recognition (CVPR), 2013 IEEE Confer-\\nence on , pages 1814–1821. IEEE, 2013. 5\\n[6] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\\nvation feature for generic visual recognition. arXiv preprint\\narXiv:1310.1531 , 2013. 4\\n[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards uniﬁed\\nobject detection and semantic segmentation. In Computer\\nVision–ECCV 2014 , pages 299–314. Springer, 2014. 7\\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\\nobject detection using deep neural networks. In Computer\\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\\nence on , pages 2155–2162. IEEE, 2014. 5, 6\\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\\nject classes challenge: A retrospective. International Journal\\nof Computer Vision , 111(1):98–136, Jan. 2015. 2\\n[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 32(9):1627–1645, 2010. 1, 4\\n[11] S. Gidaris and N. Komodakis. Object detection via a multi-\\nregion & semantic segmentation-aware CNN model. CoRR ,\\nabs/1505.01749, 2015. 7\\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\\nple in cubist art. In Computer Vision-ECCV 2014 Workshops ,\\npages 101–116. Springer, 2014. 7\\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. In Computer Vision and Pattern Recognition\\n(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\\n2014. 1, 4, 7\\n[14] R. B. Girshick. Fast R-CNN. CoRR , abs/1504.08083, 2015.\\n2, 5, 6, 7\\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\\ntion and object detection. In Advances in neural information\\nprocessing systems , pages 655–663, 2009. 4[16] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Simul-\\ntaneous detection and segmentation. In Computer Vision–\\nECCV 2014 , pages 297–312. Springer, 2014. 7\\n[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\\nin deep convolutional networks for visual recognition. arXiv\\npreprint arXiv:1406.4729 , 2014. 5\\n[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. Improving neural networks by pre-\\nventing co-adaptation of feature detectors. arXiv preprint\\narXiv:1207.0580 , 2012. 4\\n[19] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In Computer Vision–ECCV 2012 , pages\\n340–353. Springer, 2012. 6\\n[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint\\narXiv:1506.06981 , 2015. 5, 6\\n[21] R. Lienhart and J. Maydt. An extended set of haar-like fea-\\ntures for rapid object detection. In Image Processing. 2002.\\nProceedings. 2002 International Conference on , volume 1,\\npages I–900. IEEE, 2002. 4\\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR ,\\nabs/1312.4400, 2013. 2\\n[23] D. G. Lowe. Object recognition from local scale-invariant\\nfeatures. In Computer vision, 1999. The proceedings of the\\nseventh IEEE international conference on , volume 2, pages\\n1150–1157. Ieee, 1999. 4\\n[24] D. Mishkin. Models accuracy on imagenet 2012\\nval. https://github.com/BVLC/caffe/wiki/\\nModels-accuracy-on-ImageNet-2012-val . Ac-\\ncessed: 2015-10-2. 3\\n[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\\nframework for object detection. In Computer vision, 1998.\\nsixth international conference on , pages 555–562. IEEE,\\n1998. 4\\n[26] J. Redmon. Darknet: Open source neural networks in c.\\nhttp://pjreddie.com/darknet/ , 2013–2016. 3\\n[27] J. Redmon and A. Angelova. Real-time grasp detection using\\nconvolutional neural networks. CoRR , abs/1412.3128, 2014.\\n5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-\\nwards real-time object detection with region proposal net-\\nworks. arXiv preprint arXiv:1506.01497 , 2015. 5, 6, 7\\n[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object\\ndetection networks on convolutional feature maps. CoRR ,\\nabs/1504.06066, 2015. 3, 7\\n[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge. International Journal of Computer\\nVision (IJCV) , 2015. 3\\n[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with\\ndpm v5. In Computer Vision–ECCV 2014 , pages 65–79.\\nSpringer, 2014. 5, 6\\n[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localiza-\\ntion and detection using convolutional networks. CoRR ,\\nabs/1312.6229, 2013. 4, 5'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 9}, page_content='[33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps\\nfor better object detection. arXiv preprint arXiv:1409.6911 ,\\n2014. 7\\n[34] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\\nGoing deeper with convolutions. CoRR , abs/1409.4842,\\n2014. 2\\n[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\\nSmeulders. Selective search for object recognition. Inter-\\nnational journal of computer vision , 104(2):154–171, 2013.\\n4\\n[36] P. Viola and M. Jones. Robust real-time object detection.\\nInternational Journal of Computer Vision , 4:34–47, 2001. 4\\n[37] P. Viola and M. J. Jones. Robust real-time face detection.\\nInternational journal of computer vision , 57(2):137–154,\\n2004. 5\\n[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable\\npart model for object detection. In Computer Vision and Pat-\\ntern Recognition (CVPR), 2014 IEEE Conference on , pages\\n2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnick and P. Doll ´ar. Edge boxes: Locating object pro-\\nposals from edges. In Computer Vision–ECCV 2014 , pages\\n391–405. Springer, 2014. 4')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://arxiv.org/pdf/1506.02640', 'page': 0}, page_content='You Only Look Once:\\nUniﬁed, Real-Time Object Detection\\nJoseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\\nUniversity of Washington∗, Allen Institute for AI†, Facebook AI Research¶\\nhttp://pjreddie.com/yolo/\\nAbstract\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classiﬁers to per-\\nform detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and\\nassociated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from\\nfull images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end\\ndirectly on detection performance.\\nOur uniﬁed architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames\\nper second. A smaller version of the network, Fast YOLO,\\nprocesses an astounding 155 frames per second while\\nstill achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO\\nmakes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very\\ngeneral representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-\\nalizing from natural images to other domains like artwork.\\n1. Introduction\\nHumans glance at an image and instantly know what ob-\\njects are in the image, where they are, and how they inter-\\nact. The human visual system is fast and accurate, allow-\\ning us to perform complex tasks like driving with little con-\\nscious thought. Fast, accurate algorithms for object detec-\\ntion would allow computers to drive cars without special-\\nized sensors, enable assistive devices to convey real-time\\nscene information to human users, and unlock the potential\\nfor general purpose, responsive robotic systems.\\nCurrent detection systems repurpose classiﬁers to per-\\nform detection. To detect an object, these systems take a\\nclassiﬁer for that object and evaluate it at various locations\\nand scales in a test image. Systems like deformable parts\\nmodels (DPM) use a sliding window approach where the\\nclassiﬁer is run at evenly spaced locations over the entire\\nimage [10].\\nMore recent approaches like R-CNN use region proposal\\n1. Resize image.\\n2. Run convolutional network.3. Non-max suppression.\\nDog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\\nwith YOLO is simple and straightforward. Our system (1) resizes\\nthe input image to 448×448, (2) runs a single convolutional net-\\nwork on the image, and (3) thresholds the resulting detections by\\nthe model’s conﬁdence.\\nmethods to ﬁrst generate potential bounding boxes in an im-\\nage and then run a classiﬁer on these proposed boxes. After\\nclassiﬁcation, post-processing is used to reﬁne the bound-\\ning boxes, eliminate duplicate detections, and rescore the\\nboxes based on other objects in the scene [13]. These com-\\nplex pipelines are slow and hard to optimize because each\\nindividual component must be trained separately.\\nWe reframe object detection as a single regression prob-\\nlem, straight from image pixels to bounding box coordi-\\nnates and class probabilities. Using our system, you only\\nlook once (YOLO) at an image to predict what objects are\\npresent and where they are.\\nYOLO is refreshingly simple: see Figure 1. A sin-\\ngle convolutional network simultaneously predicts multi-\\nple bounding boxes and class probabilities for those boxes.\\nYOLO trains on full images and directly optimizes detec-\\ntion performance. This uniﬁed model has several beneﬁts\\nover traditional methods of object detection.\\nFirst, YOLO is extremely fast. Since we frame detection\\nas a regression problem we don’t need a complex pipeline.\\nWe simply run our neural network on a new image at test\\ntime to predict detections. Our base network runs at 45\\nframes per second with no batch processing on a Titan X\\nGPU and a fast version runs at more than 150 fps. This\\nmeans we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/ .\\nSecond, YOLO reasons globally about the image when\\n1arXiv:1506.02640v5  [cs.CV]  9 May 2016')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Load PDF File Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFDirectoryLoader(path)\n",
    "docs = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs) #25 = 10 + 15 from 2 pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using WebPage\n",
    "##### Sẽ có những trường thuộc tính của HTML, cần chọn thuộc tính muốn lấy về"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_url = [\"https://huyenchip.com/2023/10/10/multimodal.html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"post-content\", \"post-title\", \"post-header\", \"page-content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs4_strainer = bs4.SoupStrainer(class_=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_loader = WebBaseLoader(\n",
    "        web_paths=web_url,\n",
    "        bs_kwargs=dict(\n",
    "                parse_only = bs4_strainer\n",
    "        )\n",
    ")\n",
    "docs = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://huyenchip.com/2023/10/10/multimodal.html'}, page_content=\"\\n\\n\\n\\nMultimodality and Large Multimodal Models (LMMs)\\n\\n\\n        \\n        Oct 10, 2023\\n      \\n      \\n        ‚Ä¢ Chip Huyen\\n\\n\\n\\nFor a long time, each ML model operated in one data mode ‚Äì text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).\\nHowever, natural intelligence is not limited to just a single modality. Humans can read, talk, and see. We listen to music to relax and watch out for strange noises to detect danger. Being able to work with multimodal data is essential for us or any AI to operate in the real world.\\nOpenAI noted in their GPT-4V system card that ‚Äúincorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development.‚Äù\\nIncorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models). Not all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but don‚Äôt have a language model component. Multimodal can mean one or more of the following:\\n\\nInput and output are of different modalities (e.g. text-to-image, image-to-text)\\nInputs are multimodal (e.g. a system that can process both text and images)\\nOutputs are multimodal (e.g. a system that can generate both text and images)\\n\\nThis post covers multimodal systems in general, including LMMs. It consists of 3 parts.\\n\\nPart 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.\\nPart 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.\\nPart 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.\\n\\nThe post is long. Feel free to skip to the sections most interesting to you.\\n‚ö† Ambiguous terminology ‚ö†\\nMultimodal data can also refer to multimodal distributions, e.g. bimodal distribution, which is different from multimodal data in this post.\\n\\n\\nTable of contents\\nPart 1. Understanding Multimodal\\n‚Ä¶. Why multimodal\\n‚Ä¶. Data modalities\\n‚Ä¶. Multimodal tasks\\n‚Ä¶‚Ä¶.. Generation\\n‚Ä¶‚Ä¶.. Vision-language understanding\\nPart 2. Fundamentals of Multimodal Training\\n‚Ä¶. CLIP: Contrastive Language-Image Pre-training\\n‚Ä¶‚Ä¶.. CLIP‚Äôs high-level architecture\\n‚Ä¶‚Ä¶.. Natural language supervision\\n‚Ä¶‚Ä¶.. Contrastive learning\\n‚Ä¶‚Ä¶.. CLIP applications\\n‚Ä¶. Flamingo: the dawns of LMMs\\n‚Ä¶‚Ä¶.. Flamingo‚Äôs high-level architecture\\n‚Ä¶‚Ä¶.. Data\\n‚Ä¶‚Ä¶.. Flamingo‚Äôs vision encoder\\n‚Ä¶‚Ä¶.. Flamingo‚Äôs language model\\n‚Ä¶. TL;DR: CLIP vs. Flamingo\\nPart 3. Research Directions for LMMs\\n‚Ä¶. Incorporating more data modalities\\n‚Ä¶. Multimodal systems for instruction-following\\n‚Ä¶. Adapters for more efficient multimodal training\\n‚Ä¶. Generating multimodal outputs\\nConclusion\\nResources\\n\\n\\nPart 1. Understanding Multimodal\\nWhy multimodal\\nMany use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.\\n\\n\\n\\n\\n    An example of how multimodality can be used in healthcare. Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022)\\n\\n\\nNot only that, incorporating data from other modalities can help boost model performance. Shouldn‚Äôt a model that can learn from both text and images perform better than a model that can learn from only text or only image?\\nMultimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment. Imagine you can ask a question by typing, talking, or just pointing your camera at something.\\nOne use case that I‚Äôm especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.\\n\\n\\n\\n\\n    Some cool multimodal use cases from GPT-4V\\n\\n\\nData modalities\\nDifferent data modes are text, image, audio, tabular data, etc. One data mode can be represented or approximated in another data mode. For example:\\n\\nAudio can be represented as images (mel spectrograms).\\nSpeech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.\\nAn image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.\\nA video is a sequence of images plus audio. ML models today mostly treat videos as sequences of images. This is a severe limitation, as sounds have proved to be just as important as visuals for videos. 88% of TikTok users shared that sound is essential for their TikTok experience.\\nA text can be represented as an image if you simply take a picture of it.\\nA data table can be converted into a chart, which is an image.\\n\\n\\nHow about other data modalities?\\nAll digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings. A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.\\nThere are other data modalities we haven‚Äôt touched on, such as graphs and 3D assets. We also haven‚Äôt touched on the formats used to represent smell and touch (haptics).\\n\\n\\nIn ML today, audio is still largely treated as a voice-based alternative to text. The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech). Non-speech audio use cases, e.g. music generation, are still pretty niche. See the fake Drake & Weeknd song and MusicGen model on HuggingFace.\\nImage is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. There‚Äôs also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.\\nText is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.\\nFor simplicity, we‚Äôll focus on 2 modalities: images and text. The learnings can be somewhat generalized to other modalities.\\nMultimodal tasks\\nTo understand multimodal systems, it‚Äôs helpful to look at the tasks they are built to solve. In literature, I commonly see vision-language tasks divided into two groups: generation and vision-language understanding (VLU), which is the umbrella term for all tasks that don‚Äôt require generation. The line between these two groups is blurred, as being able to generate answers requires understanding too.\\nGeneration\\nFor generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal. While unimodal outputs are common today, multimodal outputs are still shaping up. We‚Äôll discuss multimodal outputs at the end of this post.\\nImage generation (text-to-image synthesis)\\nThis category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.\\nText generation\\nA common text generation task is visual question answering. Instead of relying only on text for the context, you can give the model both text and images. Imagine you can point your camera to anything and ask questions like: ‚ÄúMy car won‚Äôt start. What‚Äôs wrong with it?‚Äù, ‚ÄúHow to make this dish?‚Äù, or ‚ÄúWhat is this meme about?‚Äù.\\nAnother common use case is image captioning, which can be used as part of a text-based image retrieval system. An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc. AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.\\nVision-language understanding\\nWe‚Äôll zoom into two task types: classification and text-based image retrieval (TBIR).\\nClassification\\nClassification models can only generate outputs that belong to a pre-determined list of classes. This works when you only care about a fixed number of outcomes. For example, an OCR system only needs to predict if a visual is one of the known characters (e.g. a digit or a letter).\\nSide note: An OCR system processes data at the character level. When used together with a system that can understand the broader context, it can improve use cases such as allowing you to ‚Äútalk‚Äù to any textbook, contract, assembly instructions, etc.\\n\\n\\n\\n\\n    Document processing with GPT-4V. The model's mistake is highlighted in red.\\n\\n\\nOne related task to classification is image-to-text retrieval: given an image and a pool of pre-defined texts, find the text that‚Äôs most likely to accompany the image. This can be helpful for product image search, i.e. retrieving product reviews from a picture.\\nText-based image retrieval (image search)\\nImage search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents. Some people call text-based image retrieval ‚Äútext-to-image retrieval‚Äù.\\nThere are several approaches to text-based image retrieval. Two of them are:\\n\\nGenerate captions and metadata for each image, either manually or automatically (see image captioning in Text generation). Given a text query, find images whose captions/metadata are closest to this text query.\\nTrain a joint embedding space for both images and text. Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.\\n\\nThe second approach is more flexible, and I believe will be more widely used. This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAI‚Äôs CLIP developed.\\nPart 2. Fundamentals of Multimodal Training\\nGiven the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on. In the end, I decided to focus on two models: CLIP (2021) and Flamingo (2022) both for their significance as well as availability and clarity of public details.\\n\\nCLIP was the first model that could generalize to multiple image classification tasks with zero- and few-shot learning.\\nFlamingo wasn‚Äôt the first large multimodal model that could generate open-ended responses (Salesforce‚Äôs BLIP came out 3 months prior). However, Flamingo‚Äôs strong performance prompted some to consider it the GPT-3 moment in the multimodal domain.\\n\\nEven though these two models are older, many techniques they use are still relevant today. I hope they serve as the foundation to understanding newer models. The multimodal space is evolving repaidly, with many new ideas being developed. We‚Äôll go over these newer models in Part 3.\\nAt a high level, a multimodal system consists of the following components:\\n\\nAn encoder for each data modality to generate the embeddings for data of that modality.\\nA way to align embeddings of different modalities into the same multimodal embedding space.\\n[Generative models only] A language model to generate text responses. Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.\\n\\nIdeally, as many of these components should be pretrained and reusable as possible.\\nCLIP: Contrastive Language-Image Pre-training\\nCLIP‚Äôs key contribution is its ability to map data of different modalities, text and images, into a shared embedding space. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.\\nTraining this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve competitive zero-shot performance on many image classification tasks. This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval. Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images. It‚Äôs unclear if GPT-4V uses CLIP.\\n\\n\\n\\n\\n    Zero-shot image classification with CLIP\\n\\n\\nCLIP leveraged natural language supervision and contrastive learning, which allowed CLIP to both scale up their data and make training more efficient. We‚Äôll go over why/how these two techniques work.\\nCLIP's high-level architecture\\n\\n\\n\\n\\n    CLIP's architecture. Both encoders and projection matrices are jointly trained together from scratch. The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning). \\n\\n\\nFor the image encoder, the authors experimented with both ResNet and ViT. Their best-performing model is ViT-L/14@336px:\\n\\nLarge vision transformer (ViT-L)\\n14 patches (each image is divided into 14x14 pixel patches/sub-images)\\non 336x336 pixel input\\n\\nFor the text encoder, CLIP uses a Transformer model similar to GPT-2 but smaller. Their base model has only 63M parameters with 8 attention heads. The authors found CLIP‚Äôs performance to be less sensitive to the capacity of the text encoder.\\nEmbeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matrices \\\\(W_v\\\\) and \\\\(W_l\\\\).\\n\\nGiven an image embedding \\\\(V_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_vV_i\\\\).\\nGiven a text embedding \\\\(L_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_lL_i\\\\).\\n\\nWhen people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIP‚Äôs image encoder.\\nNatural language supervision\\nFor many years, image models were trained with manually annotated (image, text) datasets (e.g. ImageNet, MS COCO). This isn‚Äôt scalable. Manual annotation is time-consuming and expensive.\\nThe CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough. They created their own dataset ‚Äì 400M (image, text) pairs ‚Äì as follows.\\n\\nConstruct a list of 500,000 queries. Queries are common words, bigrams, and titles of popular Wikipedia articles.\\nFind images matching these queries (string and substring match). The paper mentioned this search did NOT happen on search engines but didn‚Äôt specify where. My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.\\nEach image is paired with a text that co-occurs with it (e.g. captions, comments) instead of the query since queries are too short to be descriptive.\\n\\nBecause some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.\\nContrastive learning\\nPre-CLIP, most vision-language models were trained using a classifier or language model objectives. Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.\\nWe‚Äôll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.\\nClassifier objective\\nA classifier predicts the correct class among a predetermined list of classes. This works when the output space is finite. Previous models that work with (image, text) pair datasets all had this limitation. For example, models working with ILSVRC-2012 limited themselves to 1,000 classes, and JFT-300M to 18,291 classes.\\nThis objective limits not only the model‚Äôs capacity to output meaningful responses but also its capacity for zero-shot learning. Say, if the model was trained to predict among 10 classes, it won‚Äôt work for a task that has 100 classes.\\nLanguage model objective\\nIf a classifier outputs only one class for each input, a language model outputs a sequence of classes. Each generated class is called a token. Each token is from a predetermined list, the vocabulary, of the language model.\\n\\n\\n\\n\\n    Classifier vs. language model objectives\\n\\n\\nContrastive objective\\nWhile the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult. They hypothesized that this is because the model tries to generate exactly the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.\\nFor example, in the Flickr30K dataset, each image has 5 captions provided by human annotators, and the captions for the same image can be very different.\\n\\n\\n\\n\\n\\n\\nContrastive learning is to overcome this challenge. Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.\\nFor each batch of \\\\(N\\\\) (image, text) pairs, the model generates N text embeddings and N image embeddings.\\n\\nLet \\\\(V_1, V_2, ..., V_n\\\\) be the embeddings for the \\\\(N\\\\) images.\\nLet \\\\(L_1, L_2, ..., L_n\\\\) be the embeddings for the \\\\(N\\\\) texts.\\n\\nCLIP computes the cosine similarity scores of the \\\\(N^2\\\\) possible (\\\\(V_i, L_j\\\\)) pairings. The model is trained to maximize the similarity scores of the \\\\(N\\\\) correct pairings while minimizing the scores of the \\\\(N^2 - N\\\\) incorrect pairings. For CLIP, \\\\(N = 32,768\\\\).\\n\\n\\n\\n\\n\\n\\nAnother way to look at this is that each training batch of CLIP is two classification tasks.\\n\\n\\nEach image can be paired with N possible texts, and the model tries to predict the correct one. This is the same setup as image-to-text retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:txt2im}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(L_i^TV_i\\\\beta)}{\\\\sum_j^N\\\\exp(L_i^TV_j\\\\beta)})\\\\]\\n  \\n\\nEach text can be paired with N possible images, and the model tries to predict the correct image. This is the same setup as text-to-image retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:im2txt}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(V_i^TL_i\\\\beta)}{\\\\sum_j^N\\\\exp(V_i^TL_j\\\\beta)})\\\\]\\n  \\n\\nThe sum of these two losses is minimized. \\uf8ffùõΩ is a trainable inverse temperature parameter.\\nThis is what it all looks like in pseudocode.\\n\\n\\n\\n\\n\\nCLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.\\n\\n\\n\\n\\n\\nCLIP applications\\nClassification\\nToday, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.\\n\\n\\n\\n\\n\\nText-based image retrieval\\nSince CLIP‚Äôs training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP ‚Äúdisplays significant promise for widely-applicable tasks like image retrieval or search.‚Äù However, ‚Äúon image retrieval, CLIP‚Äôs performance relative to the overall state of the art is noticeably lower.‚Äù\\nThere are attempts to use CLIP for image retrieval. For example, clip-retrieval package works as follows:\\n\\nGenerate CLIP embeddings for all your images and store them in a vector database.\\nFor each text query, generate a CLIP embedding for this text.\\nQuery in the vector database for all images whose embeddings are close to this text query embedding.\\n\\nImage generation\\nCLIP‚Äôs joint image-text embeddings are useful for image generation. Given a text prompt, DALL-E (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.\\nIn 2022, OpenAI introduced unCLIP, a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:\\n\\nCLIP is trained and frozen. The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.\\nTwo things happen at image generation:\\n    \\nUse CLIP to generate embedding for this text.\\nUse a diffusion decoder to generate images conditioned on this embedding.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText generation: visual question answering, captioning\\nCLIP authors did attempt to create a model for text generation. One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIP‚Äôs best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.\\nWhile today CLIP isn‚Äôt used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.\\nFlamingo: the dawns of LMMs\\nUnlike CLIP, Flamingo can generate text responses. In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.\\n\\n\\n\\n\\n    Flamingo can generate text responses conditioned on both text and images\\n\\n\\nFlamingo's high-level architecture\\nAt a high level, Flamingo consists of 2 parts:\\n\\nVision encoder: a CLIP-like model is trained using contrastive learning. The text encoder of this model is then discarded. The vision encoder is frozen to be used in the main model.\\nLanguage model: Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers. We‚Äôll discuss them later in this blog.\\n\\n\\n\\n\\n\\n\\n\\nData\\nFlamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset\\n\\nType\\n\\nSize\\n\\nHow\\n\\nTraining weight\\n\\n\\n\\nM3W\\n   \\nInterleaved image and text dataset\\n   \\n43M webpages\\n   \\nFor each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.\\n   \\n1.0\\n   \\n\\n\\nALIGN\\n   \\n(Image, text) pairs\\n   \\n1.8B pairs\\n   \\nTexts are alt-texts, averaging 12 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nLTIP\\n   \\n(Image, text) pairs\\n   \\n312M pairs\\n   \\nTexts are long descriptions, averaging 20.5 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nVTP\\n   \\n(Video, text) pairs\\n   \\n27M short videos\\n   \\n~22 seconds/video on average\\n   \\n0.03\\n   \\n\\n\\n\\nFlamingo's vision encoder\\nFlamingo first trains a CLIP-like model from scratch using contrastive learning. This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1B (image, text) pairs. This is 5x larger than the dataset CLIP was trained on.\\n\\nFor the text encoder, Flamingo uses BERT instead of GPT-2.\\nFor the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.\\nText and vision embeddings are meanpooled before being projected to the joint embedding space.\\n\\nFlamingo's language model\\nFlamingo uses Chinchilla as their language model. More specifically, they freeze the 9 pretrained Chinchilla LM layers. A traditional language model predicts the next text token based on the preceding text tokens. Flamingo predicts the next text token based on both the preceding text and visual tokens.\\n\\n\\n\\n\\n    Next token generation is conditioned on both text and visual tokens. Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models.\\n\\n\\nTo be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.\\nPerceiver Resampler\\nAs the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features. Perceiver Resampler converts these variable features into a consistent 64 visual outputs.\\nInterestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 √ó 320. It‚Äôs been shown that a higher test-time resolution can lead to improved performance when using CNNs.\\n\\n\\n\\n\\n\\n\\nGATED XATTN-DENSE layers\\nGATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens. Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.\\n\\n\\n\\n\\n\\n\\nLoss function\\nFlamingo computes the likelihood of text \\\\(y\\\\) conditioned on the interleaved images and videos \\\\(x\\\\).\\n\\n\\\\[p(y|x) = \\\\prod_{l=1}^N p(y_l|y_{<l}, x_{\\\\leq l})\\\\]\\n\\nThe training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, with \\\\(\\\\lambda_m\\\\) being the training weight of dataset \\\\(m\\\\).\\n\\n\\\\[\\\\sum_{m=1}^M \\\\lambda_m E_{(x, y)\\\\sim D_m} [ -\\\\sum_{l=1}^L \\\\log p(y|x)]\\\\]\\n\\nTraining\\nWhile the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. Finding the right per-dataset weights was key to performance. The weight for each dataset is in the Training weight column in the dataset table above.\\nVTP‚Äôs weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal. However, the authors noted that removing this dataset negatively affects performance on all video tasks.\\nWhile Flamingo isn‚Äôt open-sourced, there are many open-source replications of Flamingo.\\n\\nIDEFICS (HuggingFace)\\nmlfoundations/open_flamingo\\n\\nTL;DR: CLIP vs. Flamingo\\n\\n\\n\\n\\n\\n\\nPart 3. Research Directions for LMMs\\nCLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.\\nHere are a few directions that I‚Äôm excited about. This is far from an exhaustive list, both because this post has been long and because I‚Äôm still learning about the space too. If you have any pointers or suggestions, please let me know!\\nIncorporating more data modalities\\nToday, most multimodal systems work with text and images. It‚Äôs only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D. Wouldn‚Äôt it be amazing to have one shared embedding space for ALL data modalities?\\nExamples of works in this space:\\n\\nULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (Xue et al., Dec 2022)\\nImageBind: One Embedding Space To Bind Them All (Girdhar et al., May 2023)\\nNExT-GPT: Any-to-Any Multimodal Large Language Model (Wu et al., Sep 2023)\\nJeff Dean‚Äôs ambitious Pathways project (2021): its vision is to ‚Äúenable multimodal models that encompass vision, auditory, and language understanding simultaneously.‚Äù\\n\\n\\n\\n\\n\\n\\nMultimodal systems for instruction-following\\nFlamingo was trained for completion, but not for dialogue or for following instructions. (If you‚Äôre not familiar with completion vs. dialogue, check out my post on RLHF). Many people are working on building LMMs that can follow instructions and have conversations, such as:\\n\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nLaVIN: Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\n\\n\\n\\n\\n\\n    Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper\\n\\n\\nAdapters for more efficient multimodal training\\nWhile Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch. These train-from-scratch modules could be compute-intensive. Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.\\nSome works are quite promising. BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.\\nWorks in this space include:\\n\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\n[LAVIN] Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\\n\\nThe two images below are from Chunyuan Li‚Äôs Large Multimodal Models tutorial at CVPR 2023, which is, btw, an excellent tutorial.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerating multimodal outputs\\nWhile models that can process multimodal inputs are becoming the norm, multimodal output is still lagging. Many use cases require multimodal outputs. For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.\\nTo generate multimodal outputs, a model would first need to generate a shared intermediate output. One key question is what the intermediate output would look like.\\nOne option for intermediate output is text, which will then be used to generate/synthesize other actions.\\nFor example, CM3 (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images. GPT-4V generates Latex code, which can then be reconstructed as data tables.\\n\\n\\n\\n\\n    Sampled outputs from CM3\\n\\n\\n\\n\\n\\n\\n    GPT-4V generates Latex code, which can then be reconstructed as a data table\\n\\n\\nAnother option for intermediate output would be multimodal tokens. This is the option that Caiming Xiong, whose team at Salesforce has done a lot of awesome work on multimodality, told me. Each token will have a tag to denote whether it‚Äôs a text token or an image token. Image tokens will then be input into an image model like Diffusion to generate images. Text tokens will then be input into a language model.\\nGenerating Images with Multimodal Language Models (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\nIt‚Äôs been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post. There‚Äôs so much about multimodality that I‚Äôm sure there are many things that I‚Äôve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.\\nAs you see in part 3 of this post, we‚Äôre still in the early days of multimodal systems (so early that a friend told me he‚Äôs not sure if the LMM abbreviation would catch on). Yes, in most of my conversations, there‚Äôs little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models. However, keep in mind that LMMs do not make LLMs obsolete. As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM. Many labs that work on multimodal systems work on LLMs in parallel.\\nEarly reviewers\\nI‚Äôd like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: Han-chung Lee, Sam Reiswig, and Luke Metz.\\nResources\\nModels\\nAn incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!\\n\\nMicrosoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)\\nVQA: Visual Question Answering (May 2015)\\nVideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)\\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)\\n[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)\\nUnifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)\\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)\\nFlamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)\\nGIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)\\nCross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)\\nKOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)\\nPaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)\\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)\\nmPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nX-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nTowards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)\\nCheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\nShikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic (SenseTime, Jun 3, 2023)\\nMacaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)\\n\\nOther resources\\n\\n[CVPR2023 Tutorial Talk] Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4\\n\\nSlides: Large Multimodal Models\\n\\n\\n[CMU course] 11-777 MMML\\n[Open source] Salesforce‚Äôs LAVIS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyJupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
